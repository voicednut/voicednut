model = "gpt-4o"
model_reasoning_effort = "high"
[general]

provider = "openai"
model = "gpt-4o"
fallback_models = [
    "gpt-4o-mini",
    "gpt-4-turbo",
    "gpt-4",
    "gpt-3.5-turbo"
]
max_context_tokens = 131072

[chat]
# Deterministic yet flexible for code edits
temperature = 0.15
top_p = 0.95
max_tokens = 16000
stream = true
frequency_penalty = 0.05
presence_penalty = 0.0

# Reasoning configuration - essential for code quality
model_reasoning_enabled = true
model_reasoning_effort  = "high"
max_reasoning_tokens    = 12000
prefer_structured_think = true

[performance]
request_timeout_ms   = 90000
max_retries          = 5
retry_backoff_ms     = 1200
concurrent_requests  = 3
optimize_latency     = false
prefetch_suggestions = true

[coding]
enable_code_generation     = true
enable_refactoring         = true
enable_explanations        = true
enable_tests               = true
prefer_compact_responses   = false
prefer_strict_typing       = true
auto_complete              = true
max_edit_block_chars       = 8192
avoid_redundant_comments   = false
preferred_style            = "preserve"

# Deeper reasoning for coding tasks - critical for quality refactoring
coding_reasoning_enabled    = true
coding_reasoning_effort     = "high"
coding_max_reasoning_tokens = 20000

[project]
index_project_files     = true
max_index_file_size_kb  = 8192
respect_gitignore       = true
watch_for_file_changes  = true

[rate_limits]
requests_per_minute = 60
tokens_per_minute   = 300000
burst_limit         = 20

[logging]
enabled       = true
level         = "info"
log_requests  = false
log_responses = false

[reasoning]
enabled               = true
default_effort        = "high"
max_tokens            = 20000
allow_reflection      = true
allow_tool_feedback   = true
expose_debug_traces   = false
